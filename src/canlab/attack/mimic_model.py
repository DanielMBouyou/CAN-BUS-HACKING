"""Mod√®le LSTM Mimic ‚Äî G√©n√©ration de frames CAN stealth.

Architecture :
    - LSTM 2 couches, hidden_size=128
    - Entr√©e : s√©quence de 50 frames (payload bytes)
    - Sortie : prochaine frame candidate

Le mod√®le apprend le pattern temporel du trafic normal
et g√©n√®re des frames statistiquement similaires.
"""

from __future__ import annotations

import logging
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from canlab.config import ATTACK_CFG, PROJECT_ROOT

logger = logging.getLogger(__name__)

MODELS_DIR = PROJECT_ROOT / "data" / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)


class CANMimicLSTM(nn.Module):
    """LSTM pour mim√©tisme de trafic CAN.

    Apprend √† pr√©dire la prochaine frame CAN √©tant donn√©e
    une s√©quence de frames pr√©c√©dentes.
    """

    def __init__(
        self,
        input_size: int = ATTACK_CFG.input_size,
        hidden_size: int = ATTACK_CFG.hidden_size,
        num_layers: int = ATTACK_CFG.num_layers,
        output_size: int | None = None,
    ) -> None:
        super().__init__()
        if output_size is None:
            output_size = input_size

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0.0,
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_size),
            nn.Sigmoid(),  # Sortie entre 0 et 1 (bytes normalis√©s)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass.

        Args:
            x: (batch, seq_len, input_size) ‚Äî s√©quence de frames normalis√©es

        Returns:
            (batch, output_size) ‚Äî prochaine frame pr√©dite
        """
        lstm_out, _ = self.lstm(x)
        # Prendre la derni√®re sortie temporelle
        last_hidden = lstm_out[:, -1, :]
        return self.fc(last_hidden)

    def generate_frame(
        self, sequence: np.ndarray, device: str = "cpu"
    ) -> np.ndarray:
        """G√©n√®re une frame √† partir d'une s√©quence d'entr√©e.

        Args:
            sequence: (seq_len, input_size) frames normalis√©es [0, 1]

        Returns:
            (input_size,) frame g√©n√©r√©e [0, 255] bytes
        """
        self.eval()
        with torch.no_grad():
            x = torch.FloatTensor(sequence).unsqueeze(0).to(device)
            pred = self(x).squeeze(0).cpu().numpy()
        # D√©normaliser ‚Üí bytes
        return (pred * 255).clip(0, 255).astype(np.uint8)


def prepare_sequences(
    payloads: np.ndarray, seq_len: int = ATTACK_CFG.seq_len
) -> tuple[np.ndarray, np.ndarray]:
    """Pr√©pare les s√©quences d'entra√Ænement.

    Args:
        payloads: (n_frames, 8) bytes des payloads
        seq_len: longueur de la s√©quence

    Returns:
        X: (n_samples, seq_len, 8) s√©quences d'entr√©e
        Y: (n_samples, 8) frames cibles
    """
    # Normaliser entre 0 et 1
    data = payloads.astype(np.float32) / 255.0

    X, Y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i : i + seq_len])
        Y.append(data[i + seq_len])

    return np.array(X), np.array(Y)


def train_mimic_model(
    payloads: np.ndarray,
    model: CANMimicLSTM | None = None,
    epochs: int = ATTACK_CFG.epochs,
    batch_size: int = ATTACK_CFG.batch_size,
    lr: float = ATTACK_CFG.learning_rate,
    device: str = "cpu",
    save_path: Path | None = None,
) -> tuple[CANMimicLSTM, list[float]]:
    """Entra√Æne le mod√®le LSTM sur le trafic CAN normal.

    Returns:
        model: mod√®le entra√Æn√©
        losses: historique des pertes
    """
    X, Y = prepare_sequences(payloads)
    logger.info(
        "üìä Donn√©es entra√Ænement : X=%s, Y=%s", X.shape, Y.shape
    )

    dataset = TensorDataset(
        torch.FloatTensor(X),
        torch.FloatTensor(Y),
    )
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    if model is None:
        model = CANMimicLSTM()
    model = model.to(device)
    model.train()

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    losses: list[float] = []

    for epoch in range(epochs):
        epoch_loss = 0.0
        n_batches = 0
        for x_batch, y_batch in loader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()
            pred = model(x_batch)
            loss = criterion(pred, y_batch)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            n_batches += 1

        avg_loss = epoch_loss / max(n_batches, 1)
        losses.append(avg_loss)
        if (epoch + 1) % 10 == 0:
            logger.info("Epoch %d/%d ‚Äî Loss: %.6f", epoch + 1, epochs, avg_loss)

    if save_path is None:
        save_path = MODELS_DIR / "mimic_lstm.pt"
    save_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), save_path)
    logger.info("‚úÖ Mod√®le sauvegard√© : %s", save_path)

    return model, losses


def load_mimic_model(
    path: Path | None = None, device: str = "cpu"
) -> CANMimicLSTM:
    """Charge un mod√®le LSTM pr√©-entra√Æn√©."""
    if path is None:
        path = MODELS_DIR / "mimic_lstm.pt"
    model = CANMimicLSTM()
    model.load_state_dict(torch.load(path, map_location=device, weights_only=True))
    model.eval()
    logger.info("üì¶ Mod√®le charg√© : %s", path)
    return model
